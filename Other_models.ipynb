{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluate import plot_confusion_matrix,calculate_performance_metrics\n",
    "import nltk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "\n",
    "\n",
    "SEED = 123456\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "rn.seed(SEED)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec:\n",
    "    def __init__(self, tweet_file=None,tweets=None, num_features=100, min_word_count=3,context_size=7,downsampling=1e-3,seed=1,epochs=12):\n",
    "        self.file = tweet_file\n",
    "        self.tweets = tweets\n",
    "        self.num_features = num_features\n",
    "        self.min_word_count = min_word_count\n",
    "        self.context_size = context_size\n",
    "        self.downsampling = downsampling\n",
    "        self.epochs=epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def preprocess_tweets(self):\n",
    "        #Tokenizes tweets to words\n",
    "        raw_sentences = []\n",
    "        if self.file is not None:\n",
    "            tweets = open(self.file, \"r\",encoding=\"utf8\")\n",
    "        else:\n",
    "            tweets = self.tweets\n",
    "        for tweet in tweets:\n",
    "            raw_sentences.append(nltk.word_tokenize(tweet))\n",
    "        self.sentences = raw_sentences\n",
    "\n",
    "\n",
    "    def make_model(self):\n",
    "        #Train model for word2vec vectors on your dataset\n",
    "            self.tweet2vec = w2v(\n",
    "            sg = 1,\n",
    "            seed = self.seed,\n",
    "            workers = multiprocessing.cpu_count(),\n",
    "            size = self.num_features,\n",
    "            min_count = self.min_word_count,\n",
    "            window = self.context_size,\n",
    "            sample = self.downsampling\n",
    "        )\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.tweet2vec.build_vocab(self.sentences)\n",
    "        # Train the model\n",
    "        self.tweet2vec.train(self.sentences, epochs = 12, total_examples = len(self.sentences))\n",
    "\n",
    "    def run(self):\n",
    "        self.preprocess_tweets()\n",
    "        self.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/balanced_data.csv',index_col=False,sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    import re\n",
    "    HASHTAGS_REGEX = re.compile('#')\n",
    "    text = HASHTAGS_REGEX.sub('', text)\n",
    "\n",
    "    MENTIONS_REGEX = re.compile('@[^\\s]+')\n",
    "    text = MENTIONS_REGEX.sub('', text)\n",
    "    \n",
    "    LINK_REGEX = re.compile('https?://[^\\s]+')\n",
    "    text = LINK_REGEX.sub('', text)\n",
    "\n",
    "    puncs = '!\"$%^&*()_+~-={}|[]\\:\";<>,.?/'+'0123456789'\n",
    "    temp = str.maketrans(dict.fromkeys(puncs,\"\"))\n",
    "    text=text.translate(temp)\n",
    "    \n",
    "    temp= str.maketrans(dict.fromkeys(\"'`\",\"\")) #to preserve can't as cant\n",
    "    text = text.translate(temp)\n",
    "    \n",
    "    clean_text = re.sub(u'[\\u007B-\\uFFFF]','',text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.map(clean_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_classes = len(set(df.emoji))\n",
    "print(tot_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE=100\n",
    "\n",
    "w2vec = word2vec(tweets = df.text, num_features=100, min_word_count=3,context_size=5,downsampling=1e-3,seed=1,epochs=1000)\n",
    "w2vec.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar words in our vocab\n",
    "w = 'red'\n",
    "print(w2vec.tweet2vec.wv.most_similar(positive=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df.text)\n",
    "labels = list(df.emoji)\n",
    "\n",
    "N = int(0.9*len(tweets))\n",
    "\n",
    "all_train_tweets = tweets[:N]\n",
    "all_train_labels = labels[:N]\n",
    "\n",
    "test_tweets = tweets[N:]\n",
    "test_labels = labels[N:]\n",
    "\n",
    "val_N = int(0.9*len(all_train_tweets))\n",
    "\n",
    "train_tweets = all_train_tweets[:val_N]\n",
    "train_labels = all_train_labels[:val_N]\n",
    "\n",
    "val_tweets = all_train_tweets[val_N:]\n",
    "val_labels = all_train_labels[val_N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(li):\n",
    "    features = []\n",
    "    max_len = 0\n",
    "    for counter, tweet in enumerate(li):\n",
    "        avg_vec = np.zeros(w2vec.tweet2vec.wv.vector_size)\n",
    "        max_len = max(max_len, len(tweet))\n",
    "        for word in tweet:\n",
    "            if word not in w2vec.tweet2vec.wv.vocab:\n",
    "                continue\n",
    "            avg_vec = np.add(avg_vec, w2vec.tweet2vec.wv[word])\n",
    "        features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "    return np.asarray(features), max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = all_train_tweets + test_tweets\n",
    "max_length = math.ceil(sum([len(s.split(\" \")) for s in all_tweets])/len(all_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, max_len_train = get_vector(train_tweets)\n",
    "X_all_train,max_len_all_train = get_vector(all_train_tweets)\n",
    "X_test, max_len_test = get_vector(test_tweets) \n",
    "X_val, max_len_val_test = get_vector(val_tweets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_train = np_utils.to_categorical(all_train_labels, tot_classes)\n",
    "y_train = np_utils.to_categorical(train_labels, tot_classes)\n",
    "y_val = np_utils.to_categorical(val_labels, tot_classes)\n",
    "y_test = np_utils.to_categorical(test_labels, tot_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(400, activation='relu', input_dim=VECTOR_SIZE))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Dense(444, activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(22, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(tot_classes, activation='softmax'))\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.99)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "#score = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4000, batch_size=X_all_train.shape[0]//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred = model.predict_classes(X_test)\n",
    "calculate_performance_metrics(test_labels,mlp_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(random_state=0)\n",
    "svm.fit(X_all_train, all_train_labels)\n",
    "svm_pred = svm.predict(X_test)\n",
    "calculate_performance_metrics(test_labels,svm_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_all_train, all_train_labels)\n",
    "dt_pred = dt.predict(X_test)\n",
    "calculate_performance_metrics(test_labels,dt_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=0)\n",
    "rf.fit(X_all_train, all_train_labels)\n",
    "rf_pred = rf.predict(X_test)\n",
    "calculate_performance_metrics(test_labels,rf_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors = 500)\n",
    "knn.fit(X_all_train, all_train_labels)\n",
    "knn_pred = knn.predict(X_test)\n",
    "calculate_performance_metrics(test_labels,knn_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import BaseLine\n",
    "\n",
    "m = BaseLine()    \n",
    "m.fit(X_all_train, all_train_labels)\n",
    "base_pred = m.predict(X_test)\n",
    "calculate_performance_metrics(test_labels,base_pred,p_average=\"weighted\",\n",
    "                              r_average=\"weighted\",f1_average=\"weighted\",normalize_cm=\"true\",figsize=(7,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
